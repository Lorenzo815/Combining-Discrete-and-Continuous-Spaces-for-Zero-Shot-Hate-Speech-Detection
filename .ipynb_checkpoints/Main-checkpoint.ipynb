{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6626669-75c2-4a22-b209-94a44091ca23",
   "metadata": {},
   "source": [
    "This repository accompanies the article titled \"Tuning Hypothesis Creation: Combining Discrete and Continuous Spaces for Zero-Shot Hate Speech Detection.\" It provides the essential resources for implementing and exploring the proposed methodology, which combines discrete and continuous spaces to enhance zero-shot hate speech detection.\n",
    "\n",
    "# Pretrained Weights for Virtual Tokens:\n",
    "\n",
    "This section of the repository includes pretrained weights for virtual tokens. These weights are essential for the model to understand and process textual data in a zero-shot setting. They have been fine-tuned to excel in hate speech detection tasks.\n",
    "\n",
    "# Jupyter Notebook:\n",
    "\n",
    "The repository includes a detailed Jupyter notebook that serves as a comprehensive guide for users. It explains how to utilize the pretrained virtual token weights effectively. The notebook covers topics such as:\n",
    "- Loading and using the pretrained weights.\n",
    "- Providing code examples and usage scenarios for zero-shot hate speech classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd09e77-3c26-48b6-b685-5d87a12ad38b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "97f8416c-2fcf-4f78-b128-4e2868c86c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae11a51-592a-46da-bb9b-9ee1c35991a0",
   "metadata": {},
   "source": [
    "# Load Pretrained Roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e3625c4-5e20-4849-bce4-137003efbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_name = 'roberta-large-mnli'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43e32f62-90dc-4842-b67b-b78116f2d357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab803a46f31467fb20f2f6fb10491b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lorenzo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lorenzo\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae0d2a92ac0459ea583a463cee19b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e66a59424d34b55a92b26da14c863d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a00a8d02a1624ce0925f668a44dfa663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(m_name, num_labels=3).cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(m_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf0bd7-7399-4c5c-821b-acc5eea7d0e7",
   "metadata": {},
   "source": [
    "# Create hypothesis vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0791df64-f50b-418d-9840-1f8a4fada693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pretrained hypothesis\n",
    "full_optimized = t.load(\"VIRTUAL_TOKENS.pt\").detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "de7fa32a-f8df-4b09-b4d5-f76ad47edfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hateful_or_effensive = tokenizer.encode(\"this sentence contains hateful entries or offensive language\", add_special_tokens=False, return_tensors=\"pt\")\n",
    "hateful_or_effensive = model.cuda().base_model.embeddings.word_embeddings(hateful_or_effensive.cuda()).reshape([-1, 1024])\n",
    "\n",
    "# Insert hateful optimized token\n",
    "hateful_or_effensive[3] = full_optimized[3]\n",
    "# Insert or optimized token\n",
    "hateful_or_effensive[5] = full_optimized[5]\n",
    "# Insert offensive optimized token\n",
    "hateful_or_effensive[6] = full_optimized[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea116851-921f-496b-8d16-0fa7fbf08e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard hypothesis\n",
    "pure_hypothesis1 = tokenizer.encode(\"this sentence contains hateful entries or offensive language\", add_special_tokens=False, return_tensors=\"pt\")\n",
    "pure_hypothesis1 = model.cuda().base_model.embeddings.word_embeddings(pure_hypothesis1.cuda()).reshape([-1, 1024])\n",
    "\n",
    "pure_hypothesis2 = tokenizer.encode(\"this sentence contains offensive language\", add_special_tokens=False, return_tensors=\"pt\")\n",
    "pure_hypothesis2 = model.cuda().base_model.embeddings.word_embeddings(pure_hypothesis2.cuda()).reshape([-1,1024])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e680e19-190f-4a54-ac36-3a7f57c027b6",
   "metadata": {},
   "source": [
    "# Custom create entailment calcualtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "37a54c50-b8fd-47c9-a75b-86d8a9691b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_token_emb = model.cuda().base_model.embeddings.word_embeddings(t.tensor(tokenizer.sep_token_id).int().cuda())\n",
    "\n",
    "def comput_entailment(test,v_tokens_emb):\n",
    "    \n",
    "    input = t.tensor(tokenizer(test)[\"input_ids\"]).cuda()\n",
    "\n",
    "    add_virtual_tokens = t.cat([\n",
    "            sep_token_emb.reshape(1,-1).cuda(),\n",
    "            v_tokens_emb,\n",
    "            sep_token_emb.reshape(1,-1).cuda()\n",
    "        ],dim=0)\n",
    "\n",
    "    words_emb = model.base_model.embeddings.word_embeddings(input)\n",
    "\n",
    "    model_input_emb = t.cat([\n",
    "        words_emb,\n",
    "        add_virtual_tokens\n",
    "    ],dim=0)\n",
    "    \n",
    "    model_res_prev = model(inputs_embeds=model_input_emb.reshape(1,-1,1024)).logits.detach().cpu()\n",
    "    return t.nn.functional.softmax(model_res_prev, dim=1).reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c3242-2903-417f-84bf-fd22edabe29c",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "94ba0d7c-352c-4bc9-87bd-9cd21c5ec309",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_res = []\n",
    "\n",
    "test_sentences = [\n",
    "    \"Pure armenians and georgians are actually genetically nearly indistinguishable from turks and iranains.\",\n",
    "    \"She looks like a heshe fag\",\n",
    "    \"The most recent Miss Universe of Norway is a Sami from that place Karasjok where people have no emotions or anything to offer society.\",\n",
    "    \"Decent people are allowed here, so queers keep out\"\n",
    "                ]\n",
    "\n",
    "for test_sentence in test_sentences:\n",
    "\n",
    "    for hypo in zip([full_optimized, \n",
    "                     hateful_or_effensive,\n",
    "                     pure_hypothesis1, \n",
    "                     pure_hypothesis2],\n",
    "                    [\"full_optimized\",\n",
    "                     \"this sentence contains [Tuned HATEFUL] entries [Tuned OR] [Tuned OFFENSIVE] language\",\n",
    "                     \"this sentence contains hateful entries or offensive language\",\n",
    "                    \"this sentence contains offensive language\"]):\n",
    "\n",
    "        save_res.append({\n",
    "        \"test_sentence\": test_sentence,\n",
    "        \"hypothesis\":hypo[1],\n",
    "        \"entailmente result (chance of being hateful)\":float(comput_entailment(test_sentence, hypo[0])[-1]),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab6ad67-412d-4ba3-b283-d01d53f19430",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2ab5b814-ce7f-499e-99e6-4b5a51a00cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_sentence</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>entailmente result (chance of being hateful)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pure armenians and georgians are actually gene...</td>\n",
       "      <td>full_optimized</td>\n",
       "      <td>0.735219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pure armenians and georgians are actually gene...</td>\n",
       "      <td>this sentence contains [Tuned HATEFUL] entries...</td>\n",
       "      <td>0.469156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pure armenians and georgians are actually gene...</td>\n",
       "      <td>this sentence contains hateful entries or offe...</td>\n",
       "      <td>0.089403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pure armenians and georgians are actually gene...</td>\n",
       "      <td>this sentence contains offensive language</td>\n",
       "      <td>0.031590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>She looks like a heshe fag</td>\n",
       "      <td>full_optimized</td>\n",
       "      <td>0.911305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>She looks like a heshe fag</td>\n",
       "      <td>this sentence contains [Tuned HATEFUL] entries...</td>\n",
       "      <td>0.782841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>She looks like a heshe fag</td>\n",
       "      <td>this sentence contains hateful entries or offe...</td>\n",
       "      <td>0.269548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>She looks like a heshe fag</td>\n",
       "      <td>this sentence contains offensive language</td>\n",
       "      <td>0.177524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The most recent Miss Universe of Norway is a S...</td>\n",
       "      <td>full_optimized</td>\n",
       "      <td>0.933913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The most recent Miss Universe of Norway is a S...</td>\n",
       "      <td>this sentence contains [Tuned HATEFUL] entries...</td>\n",
       "      <td>0.563037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The most recent Miss Universe of Norway is a S...</td>\n",
       "      <td>this sentence contains hateful entries or offe...</td>\n",
       "      <td>0.072060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The most recent Miss Universe of Norway is a S...</td>\n",
       "      <td>this sentence contains offensive language</td>\n",
       "      <td>0.036139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Decent people are allowed here, so queers keep...</td>\n",
       "      <td>full_optimized</td>\n",
       "      <td>0.973850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Decent people are allowed here, so queers keep...</td>\n",
       "      <td>this sentence contains [Tuned HATEFUL] entries...</td>\n",
       "      <td>0.514690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Decent people are allowed here, so queers keep...</td>\n",
       "      <td>this sentence contains hateful entries or offe...</td>\n",
       "      <td>0.054962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Decent people are allowed here, so queers keep...</td>\n",
       "      <td>this sentence contains offensive language</td>\n",
       "      <td>0.091667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        test_sentence  \\\n",
       "0   Pure armenians and georgians are actually gene...   \n",
       "1   Pure armenians and georgians are actually gene...   \n",
       "2   Pure armenians and georgians are actually gene...   \n",
       "3   Pure armenians and georgians are actually gene...   \n",
       "4                          She looks like a heshe fag   \n",
       "5                          She looks like a heshe fag   \n",
       "6                          She looks like a heshe fag   \n",
       "7                          She looks like a heshe fag   \n",
       "8   The most recent Miss Universe of Norway is a S...   \n",
       "9   The most recent Miss Universe of Norway is a S...   \n",
       "10  The most recent Miss Universe of Norway is a S...   \n",
       "11  The most recent Miss Universe of Norway is a S...   \n",
       "12  Decent people are allowed here, so queers keep...   \n",
       "13  Decent people are allowed here, so queers keep...   \n",
       "14  Decent people are allowed here, so queers keep...   \n",
       "15  Decent people are allowed here, so queers keep...   \n",
       "\n",
       "                                           hypothesis  \\\n",
       "0                                      full_optimized   \n",
       "1   this sentence contains [Tuned HATEFUL] entries...   \n",
       "2   this sentence contains hateful entries or offe...   \n",
       "3           this sentence contains offensive language   \n",
       "4                                      full_optimized   \n",
       "5   this sentence contains [Tuned HATEFUL] entries...   \n",
       "6   this sentence contains hateful entries or offe...   \n",
       "7           this sentence contains offensive language   \n",
       "8                                      full_optimized   \n",
       "9   this sentence contains [Tuned HATEFUL] entries...   \n",
       "10  this sentence contains hateful entries or offe...   \n",
       "11          this sentence contains offensive language   \n",
       "12                                     full_optimized   \n",
       "13  this sentence contains [Tuned HATEFUL] entries...   \n",
       "14  this sentence contains hateful entries or offe...   \n",
       "15          this sentence contains offensive language   \n",
       "\n",
       "    entailmente result (chance of being hateful)  \n",
       "0                                       0.735219  \n",
       "1                                       0.469156  \n",
       "2                                       0.089403  \n",
       "3                                       0.031590  \n",
       "4                                       0.911305  \n",
       "5                                       0.782841  \n",
       "6                                       0.269548  \n",
       "7                                       0.177524  \n",
       "8                                       0.933913  \n",
       "9                                       0.563037  \n",
       "10                                      0.072060  \n",
       "11                                      0.036139  \n",
       "12                                      0.973850  \n",
       "13                                      0.514690  \n",
       "14                                      0.054962  \n",
       "15                                      0.091667  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(save_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
